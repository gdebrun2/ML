{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B8_NJo4Vcs32"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftlWyW-NU4C0"
   },
   "source": [
    "# 1. Multinomial Naive Bayes Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a Naive Bayes Classifier in this first part of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQIiO0_ap0y7"
   },
   "source": [
    "## 1.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv5WxDcSV231"
   },
   "source": [
    "We will use an artificial dataset to evaluate the classifier will build. We will first upload this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6ECLBJENps1Y"
   },
   "outputs": [],
   "source": [
    "def bayes_dataset(split, prefix=\"bayes\"):\n",
    "    '''\n",
    "    Arguments:\n",
    "        split (str): \"train\" or \"test\"\n",
    "    Returns:\n",
    "        X (S x N): features of each object, X[i][j] = 0/1\n",
    "        y (S): label of each object, y[i] = 0/1\n",
    "    '''\n",
    "    return (np.loadtxt(f\"{prefix}_{split}_data.txt\"), np.loadtxt(f\"{prefix}_{split}_target.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "TJwYdm80qGg_",
    "outputId": "c65f74a8-feba-419c-f111-cb5c094b0423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 20),\n",
       " (100,),\n",
       " array([[0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 1.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 1.],\n",
       "        [0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "         0., 0., 0., 1.],\n",
       "        [0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
       "         0., 0., 1., 1.]]),\n",
       " array([1., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target = bayes_dataset('train')\n",
    "data.shape, target.shape, data[:5], target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUi4eul0WK64"
   },
   "source": [
    "The training dataset $X\\in\\mathbb{R}^{N\\times 20}$, each sample has 20 Boolean-valued features (i.e., take values that are either $0$ or $1$). There are 2 class labels, also $0$ or $1$. Our goal is to use the the features to make prediction on the labels.\n",
    "\n",
    "We can think about the dataset in the following way. For example, we want to decide if we can Play golf or not based on a few weather conditions. Then\n",
    "- feature 1 is \"Rainy or not\", 1 means Rainy, 0 means not Rainy. \n",
    "- feature 2 is \"Cold or not\", 1 means Cold, 0 means Hot.\n",
    "- similar for all the other features. \n",
    "- the class label is \"Play golf or not\", 1 means Play Golf, 0 means not Play Golf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5UPId2-YGl_"
   },
   "source": [
    "## 1.2 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60PkbGLQU4C0"
   },
   "source": [
    "Consider \n",
    "- A training dataset $X\\in\\mathbb{R}^{N\\times d}$.\n",
    "- Training labels $Y\\in\\{0,1\\}^{N}$. \n",
    "- We denote $X_{j}\\in\\mathbb{R}^N$ as the $j^{th}$ feature of $X$.\n",
    "\n",
    "Recall from class, we can derive the classification rule as:\n",
    "\n",
    "------\n",
    "\\begin{aligned}\n",
    "\\hat{Y} &=\\underset{y}{\\arg \\max } P(Y=y \\mid X) & \\text{(Definition)}\\\\\n",
    "&=\\underset{y}{\\arg \\max } \\frac{P(X \\mid Y=y) P(Y=y)}{P(X)} & \\text{(Bayes rule)}\\\\\n",
    "&=\\underset{y}{\\arg \\max } P(X \\mid Y=y) P(Y=y) & \\text{(Denominator does not depend on y)}\\\\\n",
    "&=\\underset{y}{\\arg \\max } P\\left(X_1, \\ldots, X_d \\mid Y=y\\right) P(Y=y) \\\\\n",
    "&=\\underset{y}{\\arg \\max }\\left(\\prod_{j=1}^d P\\left(X_j \\mid Y=y\\right)\\right) P(Y=y) & \\text{(Conditional independence)} \\\\\n",
    "&=\\underset{y}{\\arg \\max }\\left(\\left(\\sum_{j=1}^d \\log P\\left(X_j \\mid Y=y\\right)\\right)+\\log P(Y=y)\\right) & \\text{(Logarithmic operation)}\n",
    "\\end{aligned}\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=10Kz6CLP2HX1dQt95Wtbv3bUFnyB4n00q\" width=\"800\"/> -->\n",
    "\n",
    "<br>\n",
    "\n",
    "Therefore, when classifying a new data point $x = [x^{(1)}, ..., x^{(d)}]^\\top$, we need to choose $y$ such that\n",
    "$$\\bigg[\\log p(y) + \\sum_{j=1}^d \\log p(x^{(j)}|y) \\bigg]$$ is the largest.\n",
    "\n",
    "------\n",
    "\n",
    "However, we first need to define the probabilistic models of the prior $p(y)$ and the class-conditional feature distributions $p(x^{(j)}|y)$ using the training data. In other words, we need to **train** the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_I6bDVqq61R"
   },
   "source": [
    "### 1.2.1 Task 1: Modelling the prior $p(y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVJmfsIvrZQK"
   },
   "source": [
    "For a binary classification problem, recall that maximum likelihood estimation of $p(y=\\ell)$ (the probability that we get an example belonging to class $\\ell \\in \\{0,1\\}$) is given by\n",
    "$$p(y = \\ell) = \\frac{\\sum_{i=1}^n 1[y^{(i)} = \\ell]}{n}$$\n",
    "where for a boolean predicate $q$, $1[q]$ is $1$ if $q$ is true and $0$ if $q$ is false. In other words, the estimate for $p(y = \\ell)$ is the fraction of training examples with output $\\ell$. \n",
    "\n",
    "Implement the function `priory`, that returns this estimate based on the training set. Since $p(y=0) = 1-p(y=1)$, you can recover all necessary information from $p(y=1)$. Your function should return a single value namely $p(y=1)$.  \n",
    "\n",
    "**Remark**: Try to avoid loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-fW2TCWarHO3"
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def priory(train_labels):\n",
    "    '''\n",
    "    Parameters:\n",
    "      train_labels: shape: (N,). N is the number of training examples.\n",
    "    Return:\n",
    "      log_py: a number.\n",
    "    '''\n",
    "    n = len(train_labels)\n",
    "    py = np.sum(train_labels)/n\n",
    "\n",
    "    ### START STUDENT CODE ##\n",
    "\n",
    "    ### END CODE ###\n",
    "\n",
    "    return py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eeyrYP227PBb"
   },
   "outputs": [],
   "source": [
    "# Sample Test Case\n",
    "some_labels = np.array([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1])\n",
    "some_py = priory(some_labels)\n",
    "assert (some_py == 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTJS8QD6ai9V"
   },
   "source": [
    "Let's see what is the `py` value for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDjakcxfY39u",
    "outputId": "d5147a6a-5c43-4fa9-b0e2-a626d848336a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py = priory(target)\n",
    "py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opbQSix7wwW_"
   },
   "source": [
    "### 1.2.2 Task 2: Modelling $p(x^{(j)}|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_nyRaDuxiQH"
   },
   "source": [
    "Recall that in Naive Bayes, each input feature is Boolean valued (i.e., either $0$ or $1$). Further the maximum likelihood estimate of $p(x_j=e | y = \\ell)$ (probability that the $j$th feature takes value $e \\in \\{0,1\\}$ given that the output is $\\ell$) is \n",
    "$$p(x_j = e | y = \\ell) = \\frac{\\sum_{i=1}^n 1[x^{(i)}_j = e \\wedge y^{(i)} = \\ell]}{\\sum_{i=1}^n 1[y^{(i)} = \\ell]}.$$\n",
    "In other words, $p(x_j=e | y = \\ell)$ is the fraction among all training examples that have output label $\\ell$, those that in addition have their $j$th feature take value $e$.\n",
    "\n",
    "We need to get all such conditional probabilities for each feature and each output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrWixfS1fV9R"
   },
   "source": [
    "As mentioned in class, often the training set may contain no examples where particular feature takes on a given value $e$, and when this happens, the above estimates will return $0$. This can be undesirable in many cases. Instead, in practice, one uses Laplace smoothing, where the following estimates are used.\n",
    "\n",
    "$$p(x_j = e | y = \\ell) = \\frac{1+\\sum_{i=1}^n 1[x^{(i)}_j = e \\wedge y^{(i)} = \\ell]}{2+\\sum_{i=1}^n 1[y^{(i)} = \\ell]}.$$\n",
    "\n",
    "Now, implement the `cond` function, which uses Laplace smoothing and returns a matrix like below:\n",
    "$$\\begin{bmatrix} p(x_0=1|y=0) &  p(x_0 = 1|y=1)\\\\\n",
    "p(x_1=1|y=0) & p(x_1 = 1|y=1) \\\\\n",
    "\\cdots & \\cdots \\\\\n",
    "p(x_d=1|y=0) & p(x_d = 1|y=1)\\end{bmatrix}$$\n",
    "whose shape is `(d,2)`, where `d` is the number of features in the training set.\n",
    "\n",
    "**Remark**: We don't compute $p(x_j=0|y)$ because it is equal to $1-p(x_j=1|y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "A98-4FLZrHQ9"
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def cond(train_features, train_labels):\n",
    "    '''\n",
    "    Parameters:\n",
    "      train_features: X.shape: (N, d)\n",
    "      train_labels: Y.shape: (N,)\n",
    "    Return:\n",
    "      px_y: shape: (d,2)\n",
    "    '''\n",
    "    N, d = train_features.shape\n",
    "    px_y = np.empty((d,2))\n",
    "    ### START STUDENT CODE ###\n",
    "    \n",
    "    denom1 = 2 + (N - np.sum(train_labels))\n",
    "    denom2 = 2 + np.sum(train_labels)\n",
    "\n",
    "    \n",
    "    counts1 = np.zeros(d)\n",
    "    counts2 = np.zeros(d)\n",
    "    \n",
    "    for i, features in enumerate(train_features):\n",
    "        \n",
    "        \n",
    "    \n",
    "        #print(features, train_labels[i])\n",
    "        \n",
    "        for j, feature in enumerate(features):\n",
    "            \n",
    "            if feature == 1 and train_labels[i] == 0:\n",
    "                \n",
    "                counts1[j] += 1\n",
    "                \n",
    "            elif feature == 1 and train_labels[i] == 1:\n",
    "                \n",
    "                counts2[j] += 1\n",
    " \n",
    "    \n",
    "    px_y[:,0] = (counts1 + 1) / denom1\n",
    "    px_y[:,1] = (counts2 + 1) / denom2\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    #print(np.sum(train_features, axis = 0))\n",
    "    ### END CODE ###\n",
    "    assert px_y.shape == (d, 2)\n",
    "    return px_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "SRRxCuiC83QP"
   },
   "outputs": [],
   "source": [
    "# Sample Test Case\n",
    "some_feats = np.array([[0, 1, 1, 0, 0],\n",
    "                       [1, 0, 1, 0, 1],\n",
    "                       [1, 1, 0, 1, 0, ]])\n",
    "some_labels = np.array([0, 1, 0])\n",
    "some_px_y = cond(some_feats, some_labels)\n",
    "assert np.array_equal(some_px_y.round(2), np.array([[0.5      ,  0.66666667],\n",
    "                                                    [0.75     ,  0.33333333],\n",
    "                                                    [0.5      ,  0.66666667],\n",
    "                                                    [0.5      ,  0.33333333],\n",
    "                                                    [0.25     ,  0.66666667]]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShUYw9wO9baB"
   },
   "source": [
    "Let's see what is the `px_y` value for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogFXuPoLvPsq",
    "outputId": "c72ad5c3-382e-4e5f-805b-326b5a637dc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13333333, 0.20689655],\n",
       "       [0.37333333, 0.79310345],\n",
       "       [0.82666667, 0.5862069 ],\n",
       "       [0.50666667, 0.31034483],\n",
       "       [0.2       , 0.20689655]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px_y = cond(data, target)\n",
    "px_y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vqA_pt4kBCm"
   },
   "source": [
    "### 1.2.3 Task 3: Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iw2cw57dFKey"
   },
   "source": [
    "We have now built the functions to calculate the prior $p(y)$ and $p(x_j|y)$. Using these functions, let us build a function classifies a new data point $x \\in \\{0,1\\}^d$ using these priors. The function `classify` takes \n",
    "- `py`: An estimate for $p(y = 1)$\n",
    "- `px_y`: A `(d,2)` matrix that contains estimates of $p(x_j=e | y = \\ell)$\n",
    "- `train_features`: A `(N,d)` matrix containing `N` new examples that need to be classified\n",
    "\n",
    "and returns `(N,)` array that contains the classification for each example.\n",
    "\n",
    "Recall that to classify a new data point $x = [x^{(1)}, ..., x^{(d)}]^\\top$, we need to choose $y$ such that\n",
    "$$\\bigg[\\log p(y) + \\sum_{j=1}^d \\log p(x^{(j)}|y) \\bigg]$$ is the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "Aa0_InlLrHTN"
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def classify(py, px_y, train_features):\n",
    "    '''\n",
    "    Parameters:\n",
    "      py: a number\n",
    "      px_y: shape: (d,2)\n",
    "      train_features: shape: (N, d)\n",
    "    Return:\n",
    "      pred: shape: (N,)\n",
    "    '''\n",
    "    N, d = train_features.shape\n",
    "    pred = []\n",
    "    ### START STUDENT CODE ###\n",
    "    \n",
    "    for features in train_features:\n",
    "\n",
    "        s0 = features@px_y[:,0]\n",
    "        s1 = features@px_y[:,1]\n",
    "        \n",
    "        if s0 > s1:\n",
    "            \n",
    "            pred.append(0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            pred.append(1)\n",
    "\n",
    "\n",
    "            \n",
    "    return np.array(pred)\n",
    "\n",
    "    \n",
    "    ### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Sample Test Case\n",
    "some_feats = np.array([[0, 1, 1, 0, 0],\n",
    "                       [1, 0, 1, 0, 1],\n",
    "                       [1, 1, 0, 1, 0]])\n",
    "\n",
    "some_py = 1./3\n",
    "some_cond = np.array([[1./2, 2./3],\n",
    "                     [3./4, 1./3],\n",
    "                     [1./2, 2./3],\n",
    "                     [1./2, 1./3],\n",
    "                     [1./4, 2./3]])\n",
    "\n",
    "pred = classify(some_py, some_cond, some_feats)\n",
    "print(pred)\n",
    "ans = np.array([0, 1, 0])\n",
    "assert np.array_equal(pred,ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXKtJyZs97aI"
   },
   "source": [
    "Let's see what is the the training accuracy for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeM4vr3UQbeB",
    "outputId": "9cf73c74-fee9-42e4-a981-17eec9d75832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "pred = classify(py, px_y, data).squeeze()\n",
    "print(\"Training Accuracy: {0:0.2f}%\".format(100*(pred == target).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpyEKzkc09Vg"
   },
   "source": [
    "## 1.3 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FejJscdq-oCE"
   },
   "source": [
    "Let's load the test data, and see the testing accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "wKOyrfyBzeoe"
   },
   "outputs": [],
   "source": [
    "tdata, ttarget = bayes_dataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f8jvX0UzblT",
    "outputId": "074333e5-ee1a-4225-ae33-cc07d8ceebec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 94.00%\n"
     ]
    }
   ],
   "source": [
    "pred_test = classify(py, px_y, tdata).squeeze()\n",
    "print(\"Testing Accuracy: {0:0.2f}%\".format(100*(pred_test == ttarget).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot3YTjS6biX4"
   },
   "source": [
    "# 2. Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes Classifier is a special, simpler form of the Gaussian Discriminant Analysis (GDA) that was discussed in class. Unlike the Naive Bayes method where the input features are Boolean valued, in this approach input features taken on real values. The assumption is (as in Naive Bayes and GDA) $p(y=1)$ is assumed to be distributed according to a Bernouli distribution. In addition, given the output, the input features are assumed to be distributed accordining to the Gaussian distribution. Like Naive Bayes, we assume that the conditional distribution of each feature is independent of the other features --- this simplifying assumption allows one to model the distribution of each feature conditional on the output as a *uni-variate* Gaussian as opposed to a multi-variate distribution. It simplifies the maximum likelihood analysis, which is spelt out below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZClVacyilXQ"
   },
   "source": [
    "## 2.1 Dataset\n",
    "Now let's look at a real-world case. Let's try the diabetes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z70CG_tFkQWW"
   },
   "source": [
    "### 2.1.1 Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF9DSeeUkDji"
   },
   "source": [
    "The UC Irvine's Machine Learning Data Repository Department hosts a Kaggle Competition with famous collection of data on whether a patient has diabetes (the Pima Indians dataset), originally owned by the National Institute of Diabetes and Digestive and Kidney Diseases and donated by Vincent Sigillito. \n",
    "\n",
    "You can find this data at https://www.kaggle.com/uciml/pima-indians-diabetes-database/data. The Kaggle website offers valuable visualizations of the original data dimensions in its dashboard. It is quite insightful to take the time and make sense of the data using their dashboard before applying any method to the data.\n",
    "\n",
    "**Information Summary**\n",
    "\n",
    "* **Input/Output**: This data has a set of attributes of patients, and a categorical variable telling whether the patient is diabetic or not. \n",
    "\n",
    "* **Missing Data**: For several attributes in this data set, a value of 0 may indicate a missing value of the variable. But here we just ignore that these zero values are missing values.\n",
    "\n",
    "* **Final Goal**: We want to build a classifier that can predict whether a patient has diabetes or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tuiZY_1kgkS"
   },
   "source": [
    "### 2.1.2 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApAbUVatiwtC",
    "outputId": "a1197208-8c6b-487a-bea8-459ed84ad076"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "N3S8ay5blTAO"
   },
   "outputs": [],
   "source": [
    "# Let's generate the split ourselves.\n",
    "np_random = np.random.RandomState(seed=12345)\n",
    "rand_unifs = np_random.uniform(0,1,size=df.shape[0])\n",
    "division_thresh = np.percentile(rand_unifs, 80)\n",
    "train_indicator = rand_unifs < division_thresh\n",
    "eval_indicator = rand_unifs >= division_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVKweYd7la15",
    "outputId": "1fe2c784-3150-4dd9-aab8-c1c79d7eae27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            1       85             66             29        0  26.6   \n",
       "1            8      183             64              0        0  23.3   \n",
       "2            1       89             66             23       94  28.1   \n",
       "3            0      137             40             35      168  43.1   \n",
       "4            5      116             74              0        0  25.6   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.351   31        0  \n",
       "1                     0.672   32        1  \n",
       "2                     0.167   21        0  \n",
       "3                     2.288   33        1  \n",
       "4                     0.201   30        0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[train_indicator].reset_index(drop=True)\n",
    "train_features = train_df.loc[:, train_df.columns != 'Outcome'].values\n",
    "train_labels = train_df['Outcome'].values\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "qoxw832TlgoO"
   },
   "outputs": [],
   "source": [
    "eval_df = df[eval_indicator].reset_index(drop=True)\n",
    "eval_features = eval_df.loc[:, eval_df.columns != 'Outcome'].values\n",
    "eval_labels = eval_df['Outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5oALiyulmZV",
    "outputId": "05cf2307-1cf7-4845-d9f2-ac9f53735e72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((614, 8), (614,), (154, 8), (154,))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape, eval_features.shape, eval_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-OA1fESmVbW"
   },
   "source": [
    "## 2.2 Train\n",
    "\n",
    "The prior distributions are setup as follows:\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "y \\sim \\mbox{Bernoulli}(\\phi)\\\\\n",
    "x_j | y = 0 \\sim \\mathcal{N}(\\mu_0(j),\\sigma_0(j))\\\\\n",
    "x_j | y = 1 \\sim \\mathcal{N}(\\mu_1(j),\\sigma_1(j))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We also make the Naive Bayes assumption that each feature value conditioned on the output is independent. Thus\n",
    "$$p(x|y=\\ell) = \\prod_{j=1}^d p(x_j | y = \\ell).$$\n",
    "\n",
    "Given these assumptions, we will choose the parameters $\\phi, \\{\\mu_0(j),\\mu_1(j),\\sigma_0(j),\\sigma_1(j)\\}_{j=1}^d$ such that the likelihood of the training set is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWIC1LKnU4C0"
   },
   "source": [
    "### 2.2.1 Estimating the prior $p(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6EPvAFGU4C0"
   },
   "source": [
    "Maximum likelihood analysis, when carried out, determines that the estimate for $p(y=1)$ is the same as before, i.e.,$$p(y = 1) = \\frac{\\sum_{i=1}^n 1[y^{(i)} = 1]}{n}.$$\n",
    "\n",
    "The function `priory` we have already implemented computes this quantity, and we can reuse it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "OgnHvRRmU4C1",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-540952d95c213032",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# priory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YX_f4NgU4C3"
   },
   "source": [
    "### 2.2.2 Task 4: Estimating the Gaussian means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwA9FL4YU4C3"
   },
   "source": [
    "We will now estimate the means of the Gaussian distributions that determine the value of the features, i.e., the parameters $\\{\\mu_0(j),\\mu_1(j)\\}_{j=1}^d$. Again maximum likelihood estimation will suggest a natural estimate for these parameters, i.e., the mean value of a particular feature when restricted to the samples that have a particular output.\n",
    "$$\\mu_\\ell(j) = \\frac{\\sum_{i=1}^n x^{(i)}_j 1[y^{(i)} = \\ell]}{\\sum_{i=1}^n 1[y^{(i)} = \\ell]}$$\n",
    "\n",
    "Write a function `get_mu_y` that takes the numpy arrays `train_features` and `train_labels` as input, and outputs a matrix `mu_y` of shape `(d,2)`, where `d` is the number of features.\n",
    "\n",
    "Some points regarding this task:\n",
    "\n",
    "* **You can assume that `train_features` has no missing elements in this task**.\n",
    "\n",
    "* Try and avoid the utilization of loops as much as possible. No loops are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "bI829MHPU4C3",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9482e9412e53e401",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def get_mu_y(train_features, train_labels):\n",
    "    '''\n",
    "    Parameters:\n",
    "      train_features: shape: (N,d). N is the number of training data points, and d is the number of the features. \n",
    "      train_labels: shape: (N,) \n",
    "    Return:\n",
    "      mu_y: shape: (d,2)\n",
    "    '''\n",
    "    N, d = train_features.shape\n",
    "    \n",
    "    ### START STUDENT CODE ###\n",
    "    \n",
    "    county0 = N - np.sum(train_labels)\n",
    "    county1 = np.sum(train_labels)\n",
    "    \n",
    "    col1 = np.zeros(d)\n",
    "    col2 = np.zeros(d)\n",
    "    \n",
    "    for j in range(d):\n",
    "        \n",
    "        xjs = np.sum(train_features[:,j] * train_labels)\n",
    "        \n",
    "        \n",
    "        #print(xjs/county1)\n",
    "        col2[j] = xjs/county1\n",
    "        \n",
    "        xjs = np.sum(train_features[:,j] * abs(train_labels - 1))\n",
    "        \n",
    "        \n",
    "        #print(xjs/county0)\n",
    "        col1[j] = xjs/county0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE ###\n",
    "    mu_y = np.empty((d,2))\n",
    "    mu_y[:,0] = col1\n",
    "    mu_y[:,1] = col2\n",
    "    \n",
    "    assert mu_y.shape == (d, 2)\n",
    "    return mu_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "F6KhE8xlU4C4",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-feae5e6e77107267",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample Test Case\n",
    "some_feats = np.array([[  1. ,  85. ,  66. ,  29. ,   0. ,  26.6,   0.4,  31. ],\n",
    "                       [  8. , 183. ,  64. ,   0. ,   0. ,  23.3,   0.7,  32. ],\n",
    "                       [  1. ,  89. ,  66. ,  23. ,  94. ,  28.1,   0.2,  21. ],\n",
    "                       [  0. , 137. ,  40. ,  35. , 168. ,  43.1,   2.3,  33. ],\n",
    "                       [  5. , 116. ,  74. ,   0. ,   0. ,  25.6,   0.2,  30. ]])\n",
    "some_labels = np.array([0, 1, 0, 1, 0])\n",
    "some_mu_y = get_mu_y(some_feats, some_labels)\n",
    "assert np.array_equal(some_mu_y.round(2), np.array([[  2.33,   4.  ],\n",
    "                                                    [ 96.67, 160.  ],\n",
    "                                                    [ 68.67,  52.  ],\n",
    "                                                    [ 17.33,  17.5 ],\n",
    "                                                    [ 31.33,  84.  ],\n",
    "                                                    [ 26.77,  33.2 ],\n",
    "                                                    [  0.27,   1.5 ],\n",
    "                                                    [ 27.33,  32.5 ]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7naAGzaU4C5"
   },
   "source": [
    "### 2.2.3 Task 5: Estimating the Gaussian standard deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2FbthzyU4C5"
   },
   "source": [
    "We will now estimate the standard deviations of the Gaussian distributions that determine the value of the features, i.e., the parameters $\\{\\sigma_0(j),\\sigma_1(j)\\}_{j=1}^d$. Again maximum likelihood estimation will suggest a natural estimate for these parameters, i.e., the standard deviation of a particular feature when restricted to the samples that have a particular value.\n",
    "$$(\\sigma_\\ell(j))^2 = \\frac{\\sum_{i=1}^n (x^{(i)}_j-\\mu_\\ell(j))^2 1[y^{(i)} = \\ell]}{\\sum_{i=1}^n 1[y^{(i)} = \\ell]}.$$\n",
    "In other words, $\\sigma_\\ell(j)$ is the standard deviation in the $j$th feature when restricted to examples that have output $\\ell$.\n",
    "\n",
    "Write a function `get_sigma_y` that takes the numpy arrays `train_features` and `train_labels` as input, and outputs the matrix `\\sigma_y` with shape `(d,2)`, where `d` is the number of features. \n",
    "\n",
    "Some points regarding this task:\n",
    "\n",
    "* **You can assume that `train_features` has no missing elements in this task**.\n",
    "\n",
    "* Try and avoid the utilization of loops as much as possible. No loops are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "Bp7CzUmtU4C5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-410ce572204e37df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def get_sigma_y(train_features, train_labels):\n",
    "    '''\n",
    "    Parameters:\n",
    "      train_features: shape: (N,d). N is the number of training data points, and d is the number of the features. \n",
    "      train_labels: shape: (N,) \n",
    "    Return:\n",
    "      sigma_y: shape: (d,2)\n",
    "    '''\n",
    "    N, d = train_features.shape\n",
    "    \n",
    "    ### BEGIN STUDENT CODE ###\n",
    "    \n",
    "    county0 = N - np.sum(train_labels)\n",
    "    county1 = np.sum(train_labels)\n",
    "    \n",
    "    col1 = np.zeros(d)\n",
    "    col2 = np.zeros(d)\n",
    "    mu_y = get_mu_y(train_features, train_labels)\n",
    "\n",
    "    \n",
    "    for j in range(d):\n",
    "        \n",
    "        xjs = np.sum( (train_features[:,j] - mu_y[j][1])**2 * train_labels)\n",
    "        \n",
    "\n",
    "        col2[j] = np.sqrt(xjs/county1)\n",
    "        \n",
    "        xjs = np.sum( (train_features[:,j] - mu_y[j][0])**2 * abs(train_labels - 1))\n",
    "        \n",
    "        \n",
    "\n",
    "        col1[j] = np.sqrt(xjs/county0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END CODE ###\n",
    "    sigma_y = np.empty((d,2))\n",
    "    sigma_y[:,0] = col1\n",
    "    sigma_y[:,1] = col2\n",
    "    \n",
    "    assert sigma_y.shape == (d, 2)\n",
    "    return sigma_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "JygixOE-U4C5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-347ad2c612aa195e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample Test Case\n",
    "some_feats = np.array([[  1. ,  85. ,  66. ,  29. ,   0. ,  26.6,   0.4,  31. ],\n",
    "                       [  8. , 183. ,  64. ,   0. ,   0. ,  23.3,   0.7,  32. ],\n",
    "                       [  1. ,  89. ,  66. ,  23. ,  94. ,  28.1,   0.2,  21. ],\n",
    "                       [  0. , 137. ,  40. ,  35. , 168. ,  43.1,   2.3,  33. ],\n",
    "                       [  5. , 116. ,  74. ,   0. ,   0. ,  25.6,   0.2,  30. ]])\n",
    "some_labels = np.array([0, 1, 0, 1, 0])\n",
    "some_std_y = get_sigma_y(some_feats, some_labels)\n",
    "assert np.array_equal(some_std_y.round(3), np.array([[ 1.886,  4.   ],\n",
    "                                                     [13.768, 23.   ],\n",
    "                                                     [ 3.771, 12.   ],\n",
    "                                                     [12.499, 17.5  ],\n",
    "                                                     [44.312, 84.   ],\n",
    "                                                     [ 1.027,  9.9  ],\n",
    "                                                     [ 0.094,  0.8  ],\n",
    "                                                     [ 4.497,  0.5  ]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pNm4D-4U4C6"
   },
   "source": [
    "## 2.3 Task 6: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rNhn8dIU4C6"
   },
   "source": [
    "Having computed the priors, we can use the estimate to classify a new example $x$. Recall that the MAP rule (which we also used in Naive Bayes and logistic regression), suggests that, on input $x$, we choose the output $\\ell \\in \\{0,1\\}$ that maximizes the probability $p(y = \\ell | x)$. The conditional $p(y = \\ell | x)$ can be computed using Bayes rule using the priors, and maximizing $p(y = \\ell | x)$ is equivalent to maximizing $p(x, y = \\ell)$, which can be written as \n",
    "$$\\prod_{j=1}^d p(x_j | y = \\ell)\\cdot p(y = \\ell).$$\n",
    "Again we can instead maximize the log of this expression, which can be written as\n",
    "$$\\log p(y = \\ell) + \\sum_{j=1}^d \\log p(x_j | y = \\ell).$$\n",
    "Finally, recall that the PDF of the univariate Gaussian distribution is\n",
    "$$ p(x_j | y = \\ell) = \\frac{1}{\\sqrt{2\\pi(\\sigma_\\ell(j))^2}} \\exp (\\frac{-(x_j-\\mu_\\ell(j))^2}{2(\\sigma_\\ell(j))^2}).$$\n",
    "\n",
    "Write a function `gaussian_classify` that takes the numpy arrays `train_features` (new examples to be classified), `mu_y` (estimates of the Gaussian means), `sigma_y` (estimates of Gaussian standard deviations), and  `p_y` (estimate of $p(y = 1)$) as input, then outputs the matrix `log_p_x_y` with the shape `(N, 2)`, where `N` is the number of new examples. The matrix `log_p_x_y` has the form\n",
    "\n",
    "$$\\log p_{x,y} = \\begin{bmatrix} \\bigg[\\log p(y=0) + \\sum_{j=0}^{d} \\log p(x_j^{(1)}|y=0) \\bigg] & \\bigg[\\log p(y=1) + \\sum_{j=0}^{d} \\log p(x_j^{(1)}|y=1) \\bigg] \\\\\n",
    "\\bigg[\\log p(y=0) + \\sum_{j=0}^{d} \\log p(x_j^{(2)}|y=0) \\bigg] & \\bigg[\\log p(y=1) + \\sum_{j=0}^{d} \\log p(x_j^{(2)}|y=1) \\bigg] \\\\\n",
    "\\cdots & \\cdots \\\\\n",
    "\\bigg[\\log p(y=0) + \\sum_{j=0}^{d} \\log p(x_j^{(N)}|y=0) \\bigg] & \\bigg[\\log p(y=1) + \\sum_{j=0}^{d} \\log p(x_j^{(N)}|y=1) \\bigg] \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $x^{(i)}$ is the $i$th new example in `train_features`.\n",
    "\n",
    "Try and avoid the utilization of loops as much as possible. No loops are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMK4TM1OU4C6"
   },
   "source": [
    "**Hint**: You may find it useful to take the log of the Gaussian PDF on paper before implementing the function. \n",
    "\n",
    "**Important Note**: Do not use third-party and non-standard implementations for computing $\\log p(x_i^{(j)}|y)$. Using functions that find the Gaussian PDF, and then taking their log is **numerically unstable**; the Gaussian PDF values can easily become extremely small numbers that cannot be represented using floating point standards and thus would be stored as zero. Taking the log of a zero value will throw an error. On the other hand, it is unnecessary to compute and store $p(x_i^{(j)}|y)$ in order to find $\\log p(x_i^{(j)}|y)$; **you can write $\\log p(x_i^{(j)}|y)$ as a direct function of `mu_y`, `sigma_y` and the features. This latter approach is numerically stable, and can be applied when the PDF values are much smaller than could be stored using the common standards.**\n",
    "\n",
    "**Remark**: Print the shape of each matrix when you feel confused.\n",
    "\n",
    "**Remark**: When `axis=1`, certain functions remove the second number from the matrix's `shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "dGXvMFN_U4C6",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-773a3cddb6c45cf8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def gaussian_classify(train_features, mu_y, sigma_y, py):\n",
    "    '''\n",
    "    Parameters:\n",
    "      train_features: shape: (N, d)\n",
    "      mu_y: shape: (d, 2)\n",
    "      sigma_y: shape: (d, 2)\n",
    "      py: number\n",
    "    Return:\n",
    "      log_p_x_y: shape: (N, 2). the above matrix\n",
    "      pred: shape: (N, 1). The prediction results\n",
    "    '''\n",
    "    N, d = train_features.shape\n",
    "    \n",
    "    ### BEGIN STUDENT CODE ###\n",
    "    \n",
    "    log_p_x_y = np.empty((N,2))\n",
    "    s0 = 0\n",
    "    s1 = 0\n",
    "    \n",
    "    for j in range(d):\n",
    "        \n",
    "        xjs = train_features[:,j]\n",
    "        \n",
    "        z = .5 * (xjs - mu_y[j][0])**2 / sigma_y[j][0]**2\n",
    "        \n",
    "        c = .5 * np.log(2*np.pi)\n",
    "        s0 += -np.log(sigma_y[j][0]) - c - z\n",
    "        \n",
    "        z = .5 * (xjs - mu_y[j][1])**2 / sigma_y[j][1]**2\n",
    "        \n",
    "        c = .5 * np.log(2*np.pi)\n",
    "        s1 += -np.log(sigma_y[j][1]) - c - z\n",
    "        \n",
    "    s0 += np.log(1-py)\n",
    "    s1 += np.log(py)\n",
    "    \n",
    "    log_p_x_y[:,0] = s0\n",
    "    \n",
    "    log_p_x_y[:,1] = s1\n",
    "    \n",
    "    pred = []\n",
    "    \n",
    "    for row in log_p_x_y:\n",
    "\n",
    "        s0 = row[0]\n",
    "        s1 = row[1]      \n",
    "        \n",
    "        if s0 > s1:\n",
    "            \n",
    "            pred.append(0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            pred.append(1)\n",
    "            \n",
    "    pred = np.array(pred)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    ### END CODE ###\n",
    "    \n",
    "    assert log_p_x_y.shape == (N,2)\n",
    "    assert pred.shape == (N,)\n",
    "    return log_p_x_y, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Test Case\n",
    "some_feats = np.array([[  1. ,  85. ,  66. ,  29. ,   0. ,  26.6,   0.4,  31. ],\n",
    "                       [  8. , 183. ,  64. ,   0. ,   0. ,  23.3,   0.7,  32. ],\n",
    "                       [  1. ,  89. ,  66. ,  23. ,  94. ,  28.1,   0.2,  21. ],\n",
    "                       [  0. , 137. ,  40. ,  35. , 168. ,  43.1,   2.3,  33. ],\n",
    "                       [  5. , 116. ,  74. ,   0. ,   0. ,  25.6,   0.2,  30. ]])\n",
    "some_labels = np.array([0, 1, 0, 1, 0])\n",
    "some_py = 0.4\n",
    "some_mu_y = np.array([[  2.33333333,   4.        ],\n",
    "       [ 96.66666667, 160.        ],\n",
    "       [ 68.66666667,  52.        ],\n",
    "       [ 17.33333333,  17.5       ],\n",
    "       [ 31.33333333,  84.        ],\n",
    "       [ 26.76666667,  33.2       ],\n",
    "       [  0.26666667,   1.5       ],\n",
    "       [ 27.33333333,  32.5       ]])\n",
    "some_std_y = np.array([[ 1.88561808,  4.        ],\n",
    "       [13.76791762, 23.        ],\n",
    "       [ 3.77123617, 12.        ],\n",
    "       [12.49888884, 17.5       ],\n",
    "       [44.31202495, 84.        ],\n",
    "       [ 1.02740233,  9.9       ],\n",
    "       [ 0.0942809 ,  0.8       ],\n",
    "       [ 4.49691252,  0.5       ]])\n",
    "some_log_p_x_y, some_pred = gaussian_classify(some_feats, some_mu_y, some_std_y, some_py)\n",
    "assert np.array_equal(some_log_p_x_y.round(2), np.array([[ -20.822,  -36.606],\n",
    "                                                         [ -60.879,  -27.944],\n",
    "                                                         [ -21.774, -295.68 ],\n",
    "                                                         [-417.359,  -27.944],\n",
    "                                                         [ -23.2  ,  -42.6  ]]).round(2))\n",
    "assert np.array_equal(some_pred, some_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP7jo7B2U4C8"
   },
   "source": [
    "## 2.4 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhfOo7HH6iGC"
   },
   "source": [
    "### 2.4.1 Integrate all the functions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "uXoXwK0gU4C8"
   },
   "outputs": [],
   "source": [
    "class NBClassifier():\n",
    "    def __init__(self, train_features, train_labels):\n",
    "        self.train_features = train_features\n",
    "        self.train_labels = train_labels\n",
    "        self.py = priory(train_labels)\n",
    "        self.mu_y = self.get_cc_means()\n",
    "        self.sigma_y = self.get_cc_std()\n",
    "        \n",
    "    def get_cc_means(self):\n",
    "        mu_y = get_mu_y(self.train_features, self.train_labels)\n",
    "        return mu_y\n",
    "    \n",
    "    def get_cc_std(self):\n",
    "        sigma_y = get_sigma_y(self.train_features, self.train_labels)\n",
    "        return sigma_y\n",
    "    \n",
    "    def predict(self, features):\n",
    "        _, pred = gaussian_classify(features, self.mu_y, self.sigma_y, self.py)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Jii0Tba6osc"
   },
   "source": [
    "### 2.4.2 Running our Classifier on Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "RvJyoZciU4C8"
   },
   "outputs": [],
   "source": [
    "diabetes_classifier = NBClassifier(train_features, train_labels)\n",
    "train_pred = diabetes_classifier.predict(train_features)\n",
    "eval_pred = diabetes_classifier.predict(eval_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8rVKdV9U4C8",
    "outputId": "80fb962d-5314-4aed-f6b4-017d053831f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data accuracy of your trained model is 0.7671009771986971\n",
      "The evaluation data accuracy of your trained model is 0.7532467532467533\n"
     ]
    }
   ],
   "source": [
    "train_acc = (train_pred==train_labels).mean()\n",
    "eval_acc = (eval_pred==eval_labels).mean()\n",
    "print(f'The training data accuracy of your trained model is {train_acc}')\n",
    "print(f'The evaluation data accuracy of your trained model is {eval_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teyEHip1U4C8"
   },
   "source": [
    "### 2.4.3 Running an off-the-shelf implementation of Gaussian Naive-Bayes For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uRXfqifIU4C8",
    "outputId": "105a41c0-9694-43f7-9755-f7a2c018a6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data accuracy of your trained model is 0.7671009771986971\n",
      "The evaluation data accuracy of your trained model is 0.7532467532467533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(train_features, train_labels)\n",
    "train_pred_sk = gnb.predict(train_features)\n",
    "eval_pred_sk = gnb.predict(eval_features)\n",
    "print(f'The training data accuracy of your trained model is {(train_pred_sk == train_labels).mean()}')\n",
    "print(f'The evaluation data accuracy of your trained model is {(eval_pred_sk == eval_labels).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ftlWyW-NU4C0",
    "NQIiO0_ap0y7",
    "m5UPId2-YGl_",
    "M_I6bDVqq61R",
    "opbQSix7wwW_",
    "6vqA_pt4kBCm",
    "vpyEKzkc09Vg",
    "ot3YTjS6biX4",
    "wZClVacyilXQ",
    "Z70CG_tFkQWW",
    "2tuiZY_1kgkS",
    "U-OA1fESmVbW",
    "rWIC1LKnU4C0",
    "4YX_f4NgU4C3",
    "z7naAGzaU4C5",
    "7pNm4D-4U4C6",
    "eP7jo7B2U4C8",
    "RhfOo7HH6iGC",
    "3Jii0Tba6osc",
    "teyEHip1U4C8"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
