{"cells":[{"cell_type":"markdown","metadata":{"id":"2cHqo6b1_Bzk"},"source":["# Implement a Neural Network\n","\n","This notebook contains testing code to help you develop a neural network by implementing the forward pass and backpropagation algorithm in the `models/neural_net.py` file. \n","\n","You will implement your network in the class `NeuralNetwork` inside the file `models/neural_net.py` to represent instances of the network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"nTt_CiWh_Bzm"},"outputs":[],"source":["import numpy as np\n","\n","from models.neural_net import NeuralNetwork\n","\n","# For auto-reloading external modules\n","# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\"Returns relative error\"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","metadata":{"id":"b5X9DO-5_Bzn"},"source":["The cell below initializes a toy dataset and corresponding model which will allow you to check your forward and backward pass by using a numeric gradient check. Note that we set a random seed for repeatable experiments."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"358jAXcc_Bzn"},"outputs":[],"source":["input_size = 2\n","hidden_size = 10\n","num_classes = 3\n","num_inputs = 5\n","optimizer = 'SGD'\n","\n","\n","def init_toy_model(num_layers):\n","    \"\"\"Initializes a toy model\"\"\"\n","    np.random.seed(0)\n","    hidden_sizes = [hidden_size] * (num_layers - 1)\n","    return NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, optimizer)\n","\n","def init_toy_data():\n","    \"\"\"Initializes a toy dataset\"\"\"\n","    np.random.seed(0)\n","    X = np.random.randn(num_inputs, input_size)\n","    y = np.random.randn(num_inputs, num_classes)\n","    return X, y\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zh_v9biP_Bzn"},"source":["# Implement forward and backward pass\n","\n","The first thing you will do is implement the forward pass of your neural network. The forward pass should be implemented in the `forward` function. You can use helper functions like `linear`, `relu`, and `sigmoid` to help organize your code.\n","\n","Next, you will implement the backward pass using the backpropagation algorithm. Backpropagation will compute the gradient of the loss with respect to the model parameters `W1`, `b1`, ... etc. Use an MSE for loss calcuation. Fill in the code blocks in `NeuralNetwork.backward`. "]},{"cell_type":"markdown","metadata":{"id":"GjAwpT2z_Bzo"},"source":["# Gradient  check\n","\n","If you have implemented your forward pass through the network correctly, you can use the following cell to debug your backward pass with a numeric gradient check. If your backward pass has been implemented correctly, the max relative error between your analytic solution and the numeric solution should be around 1e-7 or less for all parameters.\n"]},{"cell_type":"code","execution_count":191,"metadata":{"id":"UZM47qUP_Bzo"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5, 2)\n","(5, 3) (5, 10)\n"]},{"ename":"ValueError","evalue":"matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 3)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[191], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m net \u001b[39m=\u001b[39m init_toy_model(num)\n\u001b[1;32m     14\u001b[0m net\u001b[39m.\u001b[39mforward(X)\n\u001b[0;32m---> 15\u001b[0m net\u001b[39m.\u001b[39;49mbackward(y)\n\u001b[1;32m     16\u001b[0m gradients \u001b[39m=\u001b[39m deepcopy(net\u001b[39m.\u001b[39mgradients)\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m param_name \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39mparams:\n","File \u001b[0;32m~/Documents/Courses/CS 444/HW/assignment2/models/neural_net.py:168\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    166\u001b[0m ai \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i)]\n\u001b[1;32m    167\u001b[0m \u001b[39mprint\u001b[39m(dk\u001b[39m.\u001b[39mshape, ai\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients[\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)] \u001b[39m=\u001b[39m dk \u001b[39m@\u001b[39;49m ai \u001b[39m#np.tensordot(self.outputs[\"a\" + str(i)], dk, axes=(0, 0))\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients[\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)] \u001b[39m=\u001b[39m dk\n\u001b[1;32m    171\u001b[0m wi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n","\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 3)"]}],"source":["from copy import deepcopy\n","\n","from utils.gradient_check import eval_numerical_gradient\n","\n","X, y = init_toy_data()\n","\n","print(X.shape)\n","def f(W):\n","    net.forward(X)\n","    return net.backward(y)\n","\n","for num in [2, 3]:\n","    net = init_toy_model(num)\n","    net.forward(X)\n","    net.backward(y)\n","    gradients = deepcopy(net.gradients)\n","\n","    for param_name in net.params:\n","        param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n","        print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, gradients[param_name])))\n","    break"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"data":{"text/plain":["(3, 5)"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["w = np.ones((10, 3))\n","x = np.ones((10, 5))\n","np.tensordot(w,x, axes = (0, 0)).shape"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"shape-mismatch for sum","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[93], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m2\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m      2\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m5\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m np\u001b[39m.\u001b[39;49mtensordot(w,x, axes \u001b[39m=\u001b[39;49m (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m))\u001b[39m.\u001b[39mshape\n","File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:1110\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1108\u001b[0m             axes_b[k] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ndb\n\u001b[1;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m equal:\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mshape-mismatch for sum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1112\u001b[0m \u001b[39m# Move the axes to sum over to the end of \"a\"\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[39m# and to the front of \"b\"\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m notin \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nda) \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m axes_a]\n","\u001b[0;31mValueError\u001b[0m: shape-mismatch for sum"]}],"source":["w = np.ones((2, 10))\n","x = np.ones((5, 2))\n","np.tensordot(w,x, axes = (0, 0)).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}
