{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J0Lps5jjBnX3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCIitlWRTJgl"
   },
   "source": [
    "# Part 1: Kernel functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvBnEfFklKyk"
   },
   "source": [
    "Implement the function `poly_kernel` that defines the polynomial kernel with form $$k(x_1, x_2) = (1+x_1 \\cdot x_2)^p,$$ where $p$ is the degree of the polynomial. Notice we are taking the dot product between two training samples, and that when $p=1$, we have the linear kernel.\n",
    "\n",
    "*Hint:* you may find `np.sum` or `np.power` function helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fezsWw94ToLB"
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def poly_kernel(x1, x2, degree):\n",
    "    # x1, x2 feature vectors with shape (d,)\n",
    "    # implement the polynomial kernel function given the degree\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    #print((1 + np.inner(x1, x2))**degree)\n",
    "    return (1 +np.inner(x1, x2))**degree\n",
    "    \n",
    "    ### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ManAX5Jmby3"
   },
   "source": [
    "Implement the function `rbf_kernel` that defines the RBF (Gaussian) kernel given parameter $\\sigma$ with form $$k(x_1, x_2) = \\exp(-\\frac{\\lVert x_1-x_2 \\rVert^2}{2 \\sigma^2}).$$\n",
    "*Hint:* you may find `np.exp` or `la.norm` function helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XP7-1AtjToO1"
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def rbf_kernel(x1, x2, sigma):\n",
    "    # x1, x2 feature vectors with shape (d,)\n",
    "    # implement the rbf (Gaussian) kernel function given sigma\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    return np.exp( -la.norm(x1 - x2)**2 / ( 2* (sigma**2) ) )\n",
    "\n",
    "    \n",
    "    ### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6XAVNjGGUnHq"
   },
   "outputs": [],
   "source": [
    "#SAMPLE TEST CASE\n",
    "x1 = np.array([1, 2, 3])\n",
    "x2 = np.array([3, 4, 5])\n",
    "assert poly_kernel(x1, x2, 1) == 27\n",
    "assert poly_kernel(x1, x2, 2) == 729\n",
    "assert rbf_kernel(x1, x2, 1).round(4) == 0.0025\n",
    "assert rbf_kernel(x1, x2, 2).round(4) == 0.2231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ske6PvmKUt0v"
   },
   "source": [
    "# Part 2: Gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4JwwRHjU01F"
   },
   "source": [
    "Implement the function `Gram`, that creates the Gram matrix for a given kernel function. The two functions provided below (`poly` and `rbf`) each return their corresponding kernel function given the polynomial degree or coefficient $\\sigma$. The `Gram` function takes as input such a function `kernel`, that is already initialized with either a degree (in the polynomial case) or $\\sigma$ (in the Gaussian case). In particular, `kernel` behaves as the functions you implemented above without the coefficient parameters (i.e., they directly follow the given mathematical definitions, and only take 2 training samples as input and return a scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "D1uLrgvgWMkD"
   },
   "outputs": [],
   "source": [
    "def poly(degree):\n",
    "    return lambda x1, x2: poly_kernel(x1, x2, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pNVNT2NKWC1W"
   },
   "outputs": [],
   "source": [
    "def rbf(sigma):\n",
    "    return lambda x1, x2: rbf_kernel(x1, x2, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "hOZIOtOAVuOH"
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def Gram(x_train, kernel):\n",
    "  # x_train: 2d array with shape (n, d)\n",
    "  # kernel: The kernel function\n",
    "    n, d = x_train.shape\n",
    "  # implement the Gram matrix given the training set and a kernel function\n",
    "    G = np.empty((n,n))\n",
    "  ### START YOUR CODE ###\n",
    "    for i in range(n):\n",
    "        \n",
    "        for j in range(n):\n",
    "            \n",
    "            G[i][j] = kernel(x_train[i,:], x_train[j,:])\n",
    "            \n",
    "    return G\n",
    "\n",
    "  ### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "PSmDg87pZElj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 225.  729.  100.  169.]\n",
      " [ 729. 2601.  400.  729.]\n",
      " [ 100.  400.  196.  289.]\n",
      " [ 169.  729.  289.  484.]]\n"
     ]
    }
   ],
   "source": [
    "#SAMPLE TEST CASE\n",
    "x_train = np.array([[1, 2, 3], [3, 4, 5], [3, 0, 2], [4, 1, 2]])\n",
    "k_poly = poly(2)\n",
    "k_rbf = rbf(2)\n",
    "\n",
    "print(Gram(x_train, k_poly).round(2))\n",
    "assert np.array_equal(Gram(x_train, k_poly).round(2), np.array([[ 225.,  729.,  100.,  169.],\n",
    "                                                      [ 729., 2601.,  400.,  729.],\n",
    "                                                      [ 100.,  400.,  196.,  289.],\n",
    "                                                      [ 169.,  729.,  289.,  484.]]).round(2))\n",
    "\n",
    "assert np.array_equal(Gram(x_train, k_rbf).round(2), np.array([[1.        , 0.22313016, 0.32465247, 0.2528396 ],\n",
    "                                                               [0.22313016, 1.        , 0.04393693, 0.09301449],\n",
    "                                                               [0.32465247, 0.04393693, 1.        , 0.77880078],\n",
    "                                                               [0.2528396 , 0.09301449, 0.77880078, 1.        ]]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ0YLus1n6Ew"
   },
   "source": [
    "# Part 3: Kernalized Dual Hard-Margin SVM (from scratch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEOnNUuVale3"
   },
   "source": [
    "Recall that if the data is \n",
    "- linearly separable, we use hard-margin SVM **(Lab 6)**.\n",
    "- linearly non-separable, we use soft-margin SVM.\n",
    "\n",
    "Now we have the kernel trick, then if the data is\n",
    "- non-linearly separable, we use hard-margin kernerlized SVM **(Lab 7)**.\n",
    "- non-linearly non-separable, we use soft-margin kernerlized SVM. \n",
    "\n",
    "Intuitively, if the data $X\\in \\mathbb{R}^{n\\times d}$ is not linearly separable, we can use a function $\\phi:\\mathbb{R}^{d}â†’\\mathbb{R}^p$ so that each data point $x\\in\\mathbb{R}^d$ can be mapped into a higher dimensional space. Then the data may become linear separable in the higher dimensional space.\n",
    "\n",
    "However, such mapping will lead to much higher computational cost if the $p$ is large and the computation is carried out explicitly in the higher dimensional space. In the dual form of identifying linear classifiers, for example in the case of SVM, since the key computational elements can be reduced to computing inner products in the higher dimensional space, we can use the kernel trick to eliminate such costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDTDjFhrOFXN"
   },
   "source": [
    "## 3(a): Lagrange Dual Problem\n",
    "\n",
    "Remember from class, the Lagrange dual problem of hard-margin SVM is: \n",
    "$$\\begin{align}\n",
    "\\mathcal{L}_d (\\alpha)\n",
    "&= \\sum_{i=0}^n \\alpha_i - \\frac 12 \\sum_{i=0}^n \\sum_{k=0}^n \\alpha_i \\alpha_k y_i y_k \\phi(x_i)^T \\phi(x_k) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "__Subject to $\\forall i\\in 1..n$:__\n",
    "- $0 \\le \\alpha_i$\n",
    "- $\\sum_{i=0}^n \\alpha_i y_i = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WAN4l7jqxvk"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "gF2I1db17Poi"
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def Ld(G, alpha, y):\n",
    "    '''\n",
    "    Parameters:\n",
    "      G: Gram Matrix (n, n)\n",
    "      alpha: Lagrangian Multiplier (n, )\n",
    "      y: has shape (n, ) and has class labels {-1, 1}\n",
    "    Return:\n",
    "      Lagrange dual objective of hard-margin SVM, A Scalar\n",
    "    '''\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    t1 = np.sum(alpha)\n",
    "    \n",
    "    n = alpha.shape[0]\n",
    "    \n",
    "    t2 = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        s = 0\n",
    "        \n",
    "        for k in range(n):\n",
    "            \n",
    "            s += alpha[i] * alpha[k] * y[i] * y[k] * G[i][k]\n",
    "            \n",
    "            \n",
    "        t2 += s\n",
    "            \n",
    "    \n",
    "    return t1 - t2/2\n",
    "\n",
    "    \n",
    "    ### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "AYEzeWh77l2E"
   },
   "outputs": [],
   "source": [
    "# Sample Test Case\n",
    "np.random.seed(1234)\n",
    "X = np.array([[1, 2, 3], [3, 4, 5], [3, 0, 2], [4, 1, 2]])\n",
    "y = np.array([-1, 1, -1, 1])\n",
    "alpha = np.array([0.1, 0.4, 0.3, 0.2])\n",
    "assert np.round(Ld(Gram(X, poly(2)), alpha, y), 2) == -190.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNF0-0pFDKsu"
   },
   "source": [
    "## 3(b): Train (You will not write any code in this section)\n",
    "Now, our objective becomes to maximize $\\mathcal{L}_d (\\alpha)$. \n",
    "\n",
    "In Section 6.8 from the Ng-Ma Notes, we can use the Sequential Minimal Optimization (SMO) algorithm. But here we simply use the `optimize` function from `scipy` to compute $\\alpha$. The function returns only the *support vectors*, namely those for which $\\alpha_i > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "OwbjzVJIo17M"
   },
   "outputs": [],
   "source": [
    "def train_Ld(kernel, X, y, C):\n",
    "    N = len(y)\n",
    "    G = Gram(X, kernel)  \n",
    "    yp = y.reshape(-1, 1)\n",
    "    GramHXy = G * np.matmul(yp, yp.T) \n",
    "\n",
    "    def Ld0dAlpha(G, alpha):\n",
    "        return np.ones_like(alpha) - alpha.dot(G)\n",
    "\n",
    "    A = np.vstack((-np.eye(N), np.eye(N)))\n",
    "    b = np.hstack((np.zeros(N), C * np.ones(N)))\n",
    "    constraints = ({'type': 'eq',   'fun': lambda a: np.dot(a, y),     'jac': lambda a: y},\n",
    "                    {'type': 'ineq', 'fun': lambda a: b - np.dot(A, a), 'jac': lambda a: -A})\n",
    "\n",
    "    optRes = optimize.minimize(fun=lambda a: -Ld(G, a, y),\n",
    "                                x0=np.ones(N), \n",
    "                                method='SLSQP', \n",
    "                                jac=lambda a: -Ld0dAlpha(GramHXy, a), \n",
    "                                constraints=constraints)\n",
    "    alpha = optRes.x\n",
    "    epsilon = 1e-8\n",
    "    supportIndices = alpha > epsilon\n",
    "    supportVectors = X[supportIndices]\n",
    "    supportY = y[supportIndices]\n",
    "    supportAlpha = alpha[supportIndices]\n",
    "    return supportVectors, supportY, supportAlpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndKdsUpX7mA2"
   },
   "source": [
    "## 3(c): Test\n",
    "Since $ w = \\sum_{i=0}^n \\alpha_i y_i \\phi(x_i)$, the prediction function is now : \n",
    "$$ f(x) = sign(w^T \\phi(x) + b) = sign \\left(\\sum_{i=0}^n \\alpha_i y_i \\langle \\phi(x_i), \\phi(x) \\rangle \\right) $$\n",
    "\n",
    "Recall that that `train_Ld` function returns only the support vectors, i.e., those for which $\\alpha_i > 0$. For indices $i$ that are not in the support vector, $\\alpha_i$ is $0$ and hence those terms don't need to be added in the about equation while predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "nLbR7Uq0QPvw"
   },
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "def predict(X, k, sV, sY, sA):\n",
    "    \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "    '''\n",
    "    Parameters:\n",
    "      X: Test set, (n1, d)\n",
    "      k: kernel function, k(x, z)\n",
    "      sV: support vectors got after training, (?, d)\n",
    "      sY: support vector labels, (?,)\n",
    "      sA: corresponding alphas for support vectors, (?, d)\n",
    "    Return:\n",
    "      y_hat: predict results, (n1,)\n",
    "    '''\n",
    "    y_hat = []\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    n = sV.shape[0]\n",
    "    s = 0\n",
    "    print(X.shape)\n",
    "    print(sV.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        s += sA[i] * sY[i] * k(sV[i,:], X)\n",
    "        \n",
    " \n",
    "    y_hat.append(np.sign(s))\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    ### END CODE ###\n",
    "    return np.array(y_hat, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "w_ps18w6QTY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(4, 3)\n",
      "[[-1]]\n",
      "(1, 3)\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "# Sample Test Case\n",
    "np.random.seed(1234)\n",
    "sV = np.array([[1, 2, 3], [3, 4, 5], [3, 0, 2], [4, 1, 2]])\n",
    "sY = np.array([-1, -1, -1, 1])\n",
    "sA = np.array([0.1, 0.4, 0.3, 0.2])\n",
    "X_test = np.array([[9, 9, 9]])\n",
    "print(predict(X_test, poly(2), sV, sY, sA))\n",
    "assert predict(X_test, poly(2), sV, sY, sA) == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfL7qpXObn9p"
   },
   "source": [
    "## 3(d): Application (You will not write any code in this section.)\n",
    "\n",
    "Now, let's try our SVM on a non-linear XOR dataset. We will do this again later, but using sklearn on a more complicated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjXOHBtmh2Yv"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6n4whHnb7aQ"
   },
   "outputs": [],
   "source": [
    "def generateBatchXor(n, mu=0.5, sigma=0.5):\n",
    "    \"\"\" Four gaussian clouds in a Xor fashion \"\"\"\n",
    "    X = np.random.normal(mu, sigma, (n, 2))\n",
    "    yB0 = np.random.uniform(0, 1, n) > 0.5\n",
    "    yB1 = np.random.uniform(0, 1, n) > 0.5\n",
    "    # y is in {-1, 1}\n",
    "    y0 = 2. * yB0 - 1\n",
    "    y1 = 2. * yB1 - 1\n",
    "    X[:,0] *= y0\n",
    "    X[:,1] *= y1\n",
    "    X -= X.mean(axis=0)\n",
    "    return X, y0*y1\n",
    "\n",
    "import matplotlib.colors as pltcolors\n",
    "\n",
    "colors = ['blue','red']\n",
    "cmap = pltcolors.ListedColormap(colors)\n",
    "\n",
    "def plotLine(ax, xRange, w, x0, label, color='grey', linestyle='-', alpha=1.):\n",
    "    \"\"\" Plot a (separating) line given the normal vector (weights) and point of intercept \"\"\"\n",
    "    if type(x0) == int or type(x0) == float or type(x0) == np.float64:\n",
    "        x0 = [0, -x0 / w[1]]\n",
    "    yy = -(w[0] / w[1]) * (xRange - x0[0]) + x0[1]\n",
    "    ax.plot(xRange, yy, color=color, label=label, linestyle=linestyle)\n",
    "    \n",
    "def plotSvm(X, y, support=None, w=None, intercept=0., label='Data', separatorLabel='Separator', \n",
    "            ax=None, bound=[[-1., 1.], [-1., 1.]]):\n",
    "    \"\"\" Plot the SVM separation, and margin \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1)\n",
    "    \n",
    "    im = ax.scatter(X[:,0], X[:,1], c=y, cmap=cmap, alpha=0.5, label=label)\n",
    "    if support is not None:\n",
    "        ax.scatter(support[:,0], support[:,1], label='Support', s=80, facecolors='none', \n",
    "                   edgecolors='y', color='y')\n",
    "        print(\"Number of support vectors = %d\" % (len(support)))\n",
    "    if w is not None:\n",
    "        xx = np.array(bound[0])\n",
    "        plotLine(ax, xx, w, intercept, separatorLabel)\n",
    "        # Plot margin\n",
    "        if support is not None:\n",
    "            signedDist = np.matmul(support, w)\n",
    "            margin = np.max(signedDist) - np.min(signedDist) * np.sqrt(np.dot(w, w))\n",
    "            supportMaxNeg = support[np.argmin(signedDist)]\n",
    "            plotLine(ax, xx, w, supportMaxNeg, 'Margin -', linestyle='-.', alpha=0.8)\n",
    "            supportMaxPos = support[np.argmax(signedDist)]\n",
    "            plotLine(ax, xx, w, supportMaxPos, 'Margin +', linestyle='--', alpha=0.8)\n",
    "            ax.set_title('Margin = %.3f' % (margin))\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid()\n",
    "    ax.set_xlim(bound[0])\n",
    "    ax.set_ylim(bound[1])\n",
    "    cb = plt.colorbar(im, ax=ax)\n",
    "    loc = np.arange(-1,1,1)\n",
    "    cb.set_ticks(loc)\n",
    "    cb.set_ticklabels(['-1','1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EnrYfEqh6th"
   },
   "source": [
    "### Predict + Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "AgRPaH-3fxqA",
    "outputId": "e648a37c-4a79-41f5-e7d4-6444db404122"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "N = 100\n",
    "X_train, y_train = generateBatchXor(2*N, sigma=0.25)\n",
    "plotSvm(X_train, y_train)\n",
    "X_test, y_test = generateBatchXor(2*N, sigma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDqmW8IXiV_V",
    "outputId": "f843e8ad-13a6-4f2a-99a6-4155edc1e7da"
   },
   "outputs": [],
   "source": [
    "kernel = poly(2)\n",
    "sV, sY, sA = train_Ld(kernel, X_train, y_train, C=5)\n",
    "y_hat = predict(X_test, kernel, sV, sY, sA)\n",
    "print('Model accuracy score: {0:0.2f}%'. format(100*accuracy_score(y_test, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "JtKb65u1fpWC",
    "outputId": "fd9fb093-906e-4bcc-ec77-8dff6f1cd607"
   },
   "outputs": [],
   "source": [
    "kernel = rbf(1)\n",
    "sV, sY, sA = train_Ld(kernel, X_train, y_train, C=5)\n",
    "fig, ax = plt.subplots(1, figsize=(11, 7))\n",
    "plotSvm(X_train, y_train, support=sV, label='Training', ax=ax)\n",
    "\n",
    "# Estimate and plot decision boundary\n",
    "xx = np.linspace(-1, 1, 50)\n",
    "X0, X1 = np.meshgrid(xx, xx)\n",
    "xy = np.vstack([X0.ravel(), X1.ravel()]).T\n",
    "Y30 = predict(xy, kernel, sV, sY, sA).reshape(X0.shape)\n",
    "ax.contour(X0, X1, Y30, colors='k', levels=[-1, 0], alpha=0.3, linestyles=['-.', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12TFvZvTcvBu"
   },
   "source": [
    "# Part 4: Kernelized SVM using `sklearn` (You will not write any code in this section.)\n",
    "\n",
    "Library functions in sklearn are usually more efficient that code that you write from scratch. This part is for you to look at how these libraries can be used on some datasets and visualize those results. We will not be using the code you wrote in Part 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CUsb1xYcy4Q"
   },
   "source": [
    "------\n",
    "Now that you have implemented some kernel functions, the Gram matrix and a kernalized SVM model, we are going to look at kernelized SVMs in different contexts and examine the advantages they bring. To do this, we are going to use the sklearn.svm module that includes SVM algorithms that support kernels. You can examine the documentation and source code of various algorithms [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm); they should look familiar to you between last week's SVM lab and your work in parts 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC-Gr7AVen1_"
   },
   "source": [
    "## The XOR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRvsQa9ceqYi"
   },
   "source": [
    "A convenient and popular problem that is not linearly seperable is the XOR dataset, which is the truth table related to the XOR logical operator. Let's visualize the dataset below; confirm with yourself that this dataset is not linearly seperable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UumYYGFctfw",
    "outputId": "d57dffbe-650a-441e-8dac-c717da0197a2"
   },
   "outputs": [],
   "source": [
    "# create XOR data\n",
    "def xor_data():\n",
    "    x = np.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    return x, y\n",
    "\n",
    "# visualize XOR data\n",
    "x, y = xor_data()\n",
    "plt.scatter(x[:,0], x[:,1], c = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5tiVqfpZyeR",
    "outputId": "7f739625-b65e-4e3a-f72f-64a7d745d463"
   },
   "outputs": [],
   "source": [
    "x, y = xor_data()\n",
    "kernel = rbf(1)\n",
    "sV, sY, sA = train_Ld(kernel, x, y, C=5)\n",
    "y_hat = predict(x, kernel, sV, sY, sA)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y, y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ji6kzy1gxLB"
   },
   "source": [
    "Run the below code multiple times, that fits a linear SVM to the XOR dataset and then predicts on the XOR dataset. What do you notice? Can you explain the performance you are seeing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NszsyEphho01",
    "outputId": "eddc58a3-90cd-4ac8-f13c-2f314b361cf8"
   },
   "outputs": [],
   "source": [
    "# instantiate linear classifier\n",
    "svc=SVC(kernel = 'linear') \n",
    "\n",
    "# fit classifier to XOR data\n",
    "svc.fit(x,y)\n",
    "\n",
    "# make predictions on XOR data\n",
    "y_pred=svc.predict(x)\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c = y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq_OdTzfkcF7"
   },
   "source": [
    "Now let's try with a polynomial kernel, with just degree 2. Go ahead and run this a few times as well; what can you say about the stability of the model? Can you explain the performance here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0Rhnj9hiQNa",
    "outputId": "768dc625-c0d9-4710-a8ac-47acc7a47618"
   },
   "outputs": [],
   "source": [
    "# instantiate polynomial classifier with degree 2\n",
    "svc=SVC(kernel = 'poly', degree = 2) \n",
    "\n",
    "# fit classifier to XOR data\n",
    "svc.fit(x,y)\n",
    "\n",
    "# make predictions on XOR data\n",
    "y_pred=svc.predict(x)\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c = y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhMU7MuWjLGi"
   },
   "source": [
    "Let's try with the rbf kernel. The kernel here uses an equivalent definition with parameter $\\gamma = \\frac{1}{2 \\sigma^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sbHuo8HiTFu",
    "outputId": "d7840ab1-b0a0-41ec-809a-00437d598910"
   },
   "outputs": [],
   "source": [
    "# instantiate rbf classifier with sigma = 1\n",
    "svc=SVC(kernel = 'rbf', gamma = 0.5) # corresponds to sigma = 1\n",
    "\n",
    "# fit classifier to XOR data\n",
    "svc.fit(x,y)\n",
    "\n",
    "# make predictions on XOR data\n",
    "y_pred=svc.predict(x)\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c = y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aohoMeJjm4O"
   },
   "source": [
    "Now that we have convinced ourselves that kernel methods do actually work and can indeed provide advantages with higher dimensional datasets, let's examine a more complex dataset.\n",
    "\n",
    "**Remark**: You are welcome to check out this [website](https://jgreitemann.github.io/svm-demo). Set up some points and their classes, specify the kernel method, then see the actual SVM margins and hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jyy52qJTwGxU"
   },
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SBbmkyEwMCA"
   },
   "source": [
    "Let's look at a subset of the MNIST dataset again, which we have used to evaluate algorithms in previous labs. Even with a small subset, the dataset is complex enough that different kernel functions have noticeable performance differences. You can run the variants below to see this, which will take some time. Feel free to step away and take a break while they run. Why do you think that, in a complex, \"real\" dataset, polynomial or rbf kernels do better than linear kernels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1M06Qvvrjep"
   },
   "outputs": [],
   "source": [
    "# load the training and testing data\n",
    "train = pd.concat(map(pd.read_csv, [\"train_1.csv\",\"train_2.csv\",\"train_3.csv\",\"train_4.csv\"]), ignore_index=True)\n",
    "print(train.shape)\n",
    "test = pd.concat(map(pd.read_csv, [\"test_1.csv\",\"test_2.csv\"]), ignore_index=True)\n",
    "print(test.shape)\n",
    "train_labels = np.array(train)[:,0]\n",
    "train_data = np.array(train)[:,1:]\n",
    "test_labels = np.array(test)[:,0]\n",
    "test_data = np.array(test)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS3LohRzsJo0",
    "outputId": "031b8e01-b5bf-4744-de10-05833b52042c"
   },
   "outputs": [],
   "source": [
    "svc=SVC(kernel = 'linear')\n",
    "# fit classifier to training data\n",
    "svc.fit(train_data,train_labels)\n",
    "\n",
    "# make predictions on testing data\n",
    "y_pred=svc.predict(test_data)\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(test_labels, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DQ6PQJWuZky",
    "outputId": "9421dde3-2852-4064-c359-4fd75aac6181"
   },
   "outputs": [],
   "source": [
    "svc=SVC(kernel = 'poly', degree = 3)\n",
    "# fit classifier to training data\n",
    "svc.fit(train_data,train_labels)\n",
    "\n",
    "# make predictions on testing data\n",
    "y_pred=svc.predict(test_data)\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(test_labels, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQ10GG_3t6jx",
    "outputId": "947ba584-b816-4830-d26a-e80fa16545b7"
   },
   "outputs": [],
   "source": [
    "svc=SVC(kernel = 'rbf') # the default gamma value is \"scaled\". See documentation for details\n",
    "# fit classifier to training data\n",
    "svc.fit(train_data,train_labels)\n",
    "\n",
    "# make predictions on testing data\n",
    "y_pred=svc.predict(test_data)\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(test_labels, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "aCIitlWRTJgl",
    "ske6PvmKUt0v",
    "DZ0YLus1n6Ew",
    "12TFvZvTcvBu"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
